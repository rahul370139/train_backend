#!/usr/bin/env python3
"""
Process ONET dataset files to create a clean career database for TrainPI
"""

import pandas as pd
import numpy as np
from pathlib import Path
import re
from loguru import logger

def clean_text(text):
    """Clean and normalize text fields"""
    if pd.isna(text):
        return ""
    text = str(text).strip()
    # Remove extra whitespace and normalize
    text = re.sub(r'\s+', ' ', text)
    return text

def process_occupation_data(file_path):
    """Process occupation data file"""
    logger.info(f"Processing occupation data from {file_path}")
    
    try:
        # Read Excel file
        df = pd.read_excel(file_path)
        logger.info(f"Loaded {len(df)} occupation records")
        
        # Clean column names
        df.columns = [col.strip() for col in df.columns]
        
        # Ensure we have the required columns
        required_cols = ['O*NET-SOC Code', 'Title']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"Missing columns in occupation data: {missing_cols}")
            return None
        
        # Clean text fields
        df['Title'] = df['Title'].apply(clean_text)
        if 'Description' in df.columns:
            df['Description'] = df['Description'].apply(clean_text)
        
        # Remove rows with missing SOC codes
        df = df.dropna(subset=['O*NET-SOC Code'])
        df['O*NET-SOC Code'] = df['O*NET-SOC Code'].astype(str)
        
        logger.info(f"Processed {len(df)} occupation records")
        return df
        
    except Exception as e:
        logger.error(f"Error processing occupation data: {e}")
        return None

def process_interests_data(file_path):
    """Process interests data file - pivot to get RIASEC scores"""
    logger.info(f"Processing interests data from {file_path}")
    
    try:
        # Read Excel file
        df = pd.read_excel(file_path)
        logger.info(f"Loaded {len(df)} interest records")
        
        # Clean column names
        df.columns = [col.strip() for col in df.columns]
        
        # Ensure we have the required columns
        required_cols = ['O*NET-SOC Code', 'Element Name', 'Data Value']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"Missing columns in interests data: {missing_cols}")
            return None
        
        # Convert data value to numeric
        df['Data Value'] = pd.to_numeric(df['Data Value'], errors='coerce')
        
        # Clean text fields
        df['Element Name'] = df['Element Name'].apply(clean_text)
        
        # Remove rows with missing data
        df = df.dropna(subset=['O*NET-SOC Code', 'Element Name', 'Data Value'])
        df['O*NET-SOC Code'] = df['O*NET-SOC Code'].astype(str)
        
        # Pivot to get RIASEC scores per occupation
        # We need to identify which elements correspond to RIASEC dimensions
        # For now, we'll create dummy RIASEC scores based on the data we have
        pivoted = df.pivot_table(
            index='O*NET-SOC Code',
            columns='Element Name',
            values='Data Value',
            aggfunc='mean'
        ).reset_index()
        
        # Add dummy RIASEC scores (we'll need to map the actual elements)
        riasec_cols = ['Realistic', 'Investigative', 'Artistic', 'Social', 'Enterprising', 'Conventional']
        for col in riasec_cols:
            if col not in pivoted.columns:
                # Use random values for now - in real implementation, map actual elements
                pivoted[col] = np.random.uniform(1, 7, len(pivoted))
        
        logger.info(f"Processed {len(pivoted)} interest records")
        return pivoted
        
    except Exception as e:
        logger.error(f"Error processing interests data: {e}")
        return None

def process_skills_data(file_path):
    """Process skills data file"""
    logger.info(f"Processing skills data from {file_path}")
    
    try:
        # Read Excel file
        df = pd.read_excel(file_path)
        logger.info(f"Loaded {len(df)} skills records")
        
        # Clean column names
        df.columns = [col.strip() for col in df.columns]
        
        # Ensure we have the required columns
        required_cols = ['O*NET-SOC Code', 'Element Name', 'Data Value']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"Missing columns in skills data: {missing_cols}")
            return None
        
        # Convert data value to numeric (this will be our importance score)
        df['Data Value'] = pd.to_numeric(df['Data Value'], errors='coerce')
        
        # Clean text fields
        df['Element Name'] = df['Element Name'].apply(clean_text)
        
        # Remove rows with missing data
        df = df.dropna(subset=['O*NET-SOC Code', 'Element Name', 'Data Value'])
        df['O*NET-SOC Code'] = df['O*NET-SOC Code'].astype(str)
        
        # Rename Data Value to Importance for consistency
        df = df.rename(columns={'Data Value': 'Importance'})
        
        logger.info(f"Processed {len(df)} skills records")
        return df
        
    except Exception as e:
        logger.error(f"Error processing skills data: {e}")
        return None

def process_tasks_data(file_path):
    """Process tasks data file"""
    logger.info(f"Processing tasks data from {file_path}")
    
    try:
        # Read Excel file
        df = pd.read_excel(file_path)
        logger.info(f"Loaded {len(df)} tasks records")
        
        # Clean column names
        df.columns = [col.strip() for col in df.columns]
        
        # Ensure we have the required columns
        required_cols = ['O*NET-SOC Code', 'Task', 'Incumbents Responding']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"Missing columns in tasks data: {missing_cols}")
            return None
        
        # Convert incumbents responding to numeric (this will be our importance score)
        df['Incumbents Responding'] = pd.to_numeric(df['Incumbents Responding'], errors='coerce')
        
        # Clean text fields
        df['Task'] = df['Task'].apply(clean_text)
        
        # Remove rows with missing data
        df = df.dropna(subset=['O*NET-SOC Code', 'Task', 'Incumbents Responding'])
        df['O*NET-SOC Code'] = df['O*NET-SOC Code'].astype(str)
        
        # Rename columns for consistency
        df = df.rename(columns={'Task': 'Task Statement', 'Incumbents Responding': 'Importance'})
        
        logger.info(f"Processed {len(df)} tasks records")
        return df
        
    except Exception as e:
        logger.error(f"Error processing tasks data: {e}")
        return None

def create_top_skills_per_occupation(skills_df):
    """Create top 3 skills per occupation"""
    logger.info("Creating top skills per occupation")
    
    try:
        # Sort by importance and get top 3 skills per occupation
        top_skills = (
            skills_df.sort_values(["O*NET-SOC Code", "Importance"], ascending=[True, False])
            .groupby("O*NET-SOC Code")
            .head(3)
            .groupby("O*NET-SOC Code")["Element Name"]
            .apply(lambda x: ", ".join(x))
            .reset_index(name="top_skills")
        )
        
        logger.info(f"Created top skills for {len(top_skills)} occupations")
        return top_skills
        
    except Exception as e:
        logger.error(f"Error creating top skills: {e}")
        return pd.DataFrame()

def create_day_in_life_per_occupation(tasks_df):
    """Create day-in-life descriptions from top tasks"""
    logger.info("Creating day-in-life descriptions")
    
    try:
        # Sort by importance and get top 2 tasks per occupation
        top_tasks = (
            tasks_df.sort_values(["O*NET-SOC Code", "Importance"], ascending=[True, False])
            .groupby("O*NET-SOC Code")
            .head(2)
            .groupby("O*NET-SOC Code")["Task Statement"]
            .apply(lambda x: " ".join(x))
            .reset_index(name="day_in_life")
        )
        
        logger.info(f"Created day-in-life for {len(top_tasks)} occupations")
        return top_tasks
        
    except Exception as e:
        logger.error(f"Error creating day-in-life: {e}")
        return pd.DataFrame()

def filter_tech_occupations(df):
    """Filter to tech-relevant occupations"""
    logger.info("Filtering to tech-relevant occupations")
    
    tech_keywords = [
        "software", "computer", "data", "security", "engineer", "analyst", 
        "developer", "programmer", "system", "network", "database", "web",
        "information", "technology", "it", "cyber", "digital", "cloud",
        "machine learning", "artificial intelligence", "ai", "ml"
    ]
    
    # Create pattern for matching
    pattern = "|".join(tech_keywords)
    
    # Filter based on title
    tech_df = df[df["title"].str.lower().str.contains(pattern, na=False)]
    
    logger.info(f"Filtered to {len(tech_df)} tech-relevant occupations")
    return tech_df

def add_salary_estimates(df):
    """Add estimated salary ranges based on occupation level"""
    logger.info("Adding estimated salary ranges")
    
    # Define salary ranges based on occupation complexity
    def get_salary_range(title):
        title_lower = title.lower()
        
        # Senior/Lead positions
        if any(word in title_lower for word in ["senior", "lead", "principal", "architect", "manager"]):
            return 90000, 150000
        
        # Mid-level positions
        elif any(word in title_lower for word in ["engineer", "developer", "analyst", "specialist"]):
            return 70000, 120000
        
        # Entry-level positions
        elif any(word in title_lower for word in ["assistant", "junior", "trainee", "intern"]):
            return 45000, 75000
        
        # Default mid-range
        else:
            return 60000, 100000
    
    # Apply salary estimates
    salary_ranges = df['title'].apply(get_salary_range)
    df['salary_low'] = [range[0] for range in salary_ranges]
    df['salary_high'] = [range[1] for range in salary_ranges]
    
    return df

def add_growth_estimates(df):
    """Add estimated growth percentages"""
    logger.info("Adding estimated growth percentages")
    
    # Define growth estimates based on occupation
    def get_growth_rate(title):
        title_lower = title.lower()
        
        # High-growth areas
        if any(word in title_lower for word in ["machine learning", "ai", "data scientist", "cyber"]):
            return 35.0
        
        # Medium-growth areas
        elif any(word in title_lower for word in ["software", "developer", "engineer"]):
            return 25.0
        
        # Standard growth
        else:
            return 15.0
    
    df['growth_pct'] = df['title'].apply(get_growth_rate)
    return df

def main():
    """Main processing function"""
    logger.info("Starting dataset processing")
    
    # Define file paths
    dataset_dir = Path("../dataset")
    output_dir = Path("data")
    output_dir.mkdir(exist_ok=True)
    
    # File paths
    occupation_file = dataset_dir / "Occupation Data.xlsx"
    interests_file = dataset_dir / "Interests.xlsx"
    skills_file = dataset_dir / "Skills.xlsx"
    tasks_file = dataset_dir / "Task Statements.xlsx"
    
    # Check if files exist
    for file_path in [occupation_file, interests_file, skills_file, tasks_file]:
        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return
    
    # Process each dataset
    occ_df = process_occupation_data(occupation_file)
    int_df = process_interests_data(interests_file)
    skills_df = process_skills_data(skills_file)
    tasks_df = process_tasks_data(tasks_file)
    
    if occ_df is None or int_df is None or skills_df is None or tasks_df is None:
        logger.error("Failed to process one or more datasets")
        return
    
    # Merge datasets
    logger.info("Merging datasets")
    df = occ_df.merge(int_df, on="O*NET-SOC Code", how="inner")
    
    # Create top skills
    top_skills = create_top_skills_per_occupation(skills_df)
    df = df.merge(top_skills, on="O*NET-SOC Code", how="left")
    
    # Create day-in-life
    day_in_life = create_day_in_life_per_occupation(tasks_df)
    df = df.merge(day_in_life, on="O*NET-SOC Code", how="left")
    
    # Select and rename columns
    df = df[[
        "O*NET-SOC Code", "Title", 
        "Realistic", "Investigative", "Artistic", "Social", "Enterprising", "Conventional",
        "top_skills", "day_in_life"
    ]]
    
    df.columns = [
        "soc_code", "title",
        "realistic", "investigative", "artistic", "social", "enterprising", "conventional",
        "top_skills", "day_in_life"
    ]
    
    # Filter to tech-relevant occupations
    df = filter_tech_occupations(df)
    
    # Add salary and growth estimates
    df = add_salary_estimates(df)
    df = add_growth_estimates(df)
    
    # Fill missing values
    df['top_skills'] = df['top_skills'].fillna("technical skills, problem solving, communication")
    df['day_in_life'] = df['day_in_life'].fillna("Perform technical work, solve problems, collaborate with team.")
    
    # Z-scale interest columns
    interest_cols = ["realistic", "investigative", "artistic", "social", "enterprising", "conventional"]
    for col in interest_cols:
        df[col] = (df[col] - df[col].mean()) / df[col].std()
    
    # Save to CSV
    output_file = output_dir / "onet_bls_trimmed.csv"
    df.to_csv(output_file, index=False)
    
    logger.info(f"Processing complete! Created {len(df)} career records")
    logger.info(f"Output saved to: {output_file}")
    
    # Print summary
    print(f"\nðŸ“Š Dataset Processing Summary:")
    print(f"âœ… Total careers processed: {len(df)}")
    print(f"âœ… Tech-relevant occupations: {len(df)}")
    print(f"âœ… Salary ranges: ${df['salary_low'].min():,} - ${df['salary_high'].max():,}")
    print(f"âœ… Growth rates: {df['growth_pct'].min():.1f}% - {df['growth_pct'].max():.1f}%")
    print(f"âœ… Output file: {output_file}")

if __name__ == "__main__":
    main() 